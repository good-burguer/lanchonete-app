name: CI Dev - Build, Push & Deploy

on:
  push:
    branches: [ "main" ]
  workflow_dispatch: {}

permissions:
  id-token: write    # OIDC
  contents: read

env:
  AWS_ACCOUNT_ID: ${{ vars.AWS_ACCOUNT_ID }}
  AWS_REGION: ${{ vars.AWS_REGION }}
  AWS_ROLE_TO_ASSUME: ${{ vars.AWS_ROLE_TO_ASSUME }}
  ECR_REPOSITORY: ${{ vars.ECR_REPOSITORY }}
  EKS_CLUSTER_NAME: ${{ vars.EKS_CLUSTER_NAME }}
  K8S_NAMESPACE: ${{ vars.K8S_NAMESPACE }}
  IMAGE_TAG: ${{ github.sha }}-amd64
  IMAGE_URI: ${{ vars.AWS_ACCOUNT_ID }}.dkr.ecr.${{ vars.AWS_REGION }}.amazonaws.com/${{ vars.ECR_REPOSITORY }}:${{ github.sha }}-amd64

jobs:
  build-push-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS creds (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Wait EKS ACTIVE & grant cluster admin to GHA role
        run: |
          set -euo pipefail
          # aguarda o cluster ficar ACTIVE
          aws eks wait cluster-active --name "$EKS_CLUSTER_NAME" --region "$AWS_REGION"

          PRINCIPAL="$AWS_ROLE_TO_ASSUME"

          # Cria o Access Entry (idempotente)
          aws eks create-access-entry \
            --cluster-name "$EKS_CLUSTER_NAME" \
            --principal-arn "$PRINCIPAL" \
            --type STANDARD \
            --region "$AWS_REGION" || true

          # Associa a policy de admin no cluster (idempotente)
          aws eks associate-access-policy \
            --cluster-name "$EKS_CLUSTER_NAME" \
            --principal-arn "$PRINCIPAL" \
            --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy \
            --access-scope type=cluster \
            --region "$AWS_REGION" || true

      - name: Ensure ECR repository exists (idempotent)
        run: |
          aws ecr describe-repositories --repository-names "$ECR_REPOSITORY" >/dev/null 2>&1 || \
          aws ecr create-repository --repository-name "$ECR_REPOSITORY" >/dev/null

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build & Push (linux/amd64)
        run: |
          docker buildx build \
            --platform linux/amd64 \
            -t "$IMAGE_URI" \
            --push .

      - name: Install kubectl
        run: |
          curl -sSLo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Update kubeconfig (EKS)
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "$EKS_CLUSTER_NAME" --region "$AWS_REGION"
          # sanity check de credenciais e acesso
          kubectl cluster-info

      - name: Get GitHub App token (read lanchonete-infra)
        id: app_token
        uses: actions/create-github-app-token@v1
        with:
          app-id: ${{ vars.CI_APP_ID }}
          private-key: ${{ secrets.CI_APP_PRIVATE_KEY }}
          owner: good-burguer

      - name: Checkout infra repo (app manifests + db-migrate.yaml)
        uses: actions/checkout@v4
        with:
          token: ${{ steps.app_token.outputs.token }}
          repository: good-burguer/lanchonete-infra
          ref: main
          path: infra
          sparse-checkout: |
            k8s/app
            k8s/jobs/db-migrate.yaml
          sparse-checkout-cone-mode: false

      - name: Ensure namespace & apply app manifests
        env:
          K8S_NAMESPACE: ${{ env.K8S_NAMESPACE }}
        run: |
          set -euo pipefail
          kubectl get ns "$K8S_NAMESPACE" || kubectl create ns "$K8S_NAMESPACE"
          kubectl -n "$K8S_NAMESPACE" apply -f infra/k8s/app/
          echo "Deployments no namespace $K8S_NAMESPACE:"
          kubectl -n "$K8S_NAMESPACE" get deploy -o wide

      - name: Deploy - set image & wait rollout
        env:
          K8S_NAMESPACE: ${{ env.K8S_NAMESPACE }}
          IMAGE_URI: ${{ env.IMAGE_URI }}
        run: |
          set -euo pipefail

          echo "Verificando se o Deployment lanchonete-app existe no namespace $K8S_NAMESPACE..."
          if ! kubectl -n "$K8S_NAMESPACE" get deploy lanchonete-app >/dev/null 2>&1; then
            echo "ERRO: Deployment 'lanchonete-app' não encontrado no namespace '$K8S_NAMESPACE'."
            echo "Deployments disponíveis:" && kubectl -n "$K8S_NAMESPACE" get deploy -o wide || true
            echo "Certifique-se de que os manifests em infra/k8s/app criam um Deployment chamado 'lanchonete-app'."
            exit 1
          fi

          echo "Atualizando imagem do Deployment..."
          kubectl -n "$K8S_NAMESPACE" set image deployment/lanchonete-app app="$IMAGE_URI"
          kubectl -n "$K8S_NAMESPACE" rollout status deployment/lanchonete-app --timeout=180s

      - name: Install yq (YAML CLI)
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          yq --version

      - name: DB Migrate - render manifest (set image)
        run: |
          set -euo pipefail
          FILE="infra/k8s/jobs/db-migrate.yaml"

          echo "---- HEAD $FILE ----"
          sed -n '1,120p' "$FILE"

          # valida se é mesmo um Job; evita aplicar HTML/erro
          if ! yq '.kind' "$FILE" | grep -q '^Job$'; then
            echo "ERRO: $FILE não parece um Job válido (kind != Job)"
            exit 1
          fi

          # altera a imagem apenas no documento do Job certo
          yq -i '
            select(.kind == "Job" and .metadata.name == "db-migrate")
            | .spec.template.spec.containers[0].image = strenv(IMAGE_URI)
          ' "$FILE"

          echo "---- $FILE (renderizado) ----"
          cat "$FILE"

      - name: DB Migrate - apply and wait
        run: |
          set -euo pipefail
          FILE="infra/k8s/jobs/db-migrate.yaml"

          kubectl -n "$K8S_NAMESPACE" delete job db-migrate --ignore-not-found=true
          kubectl -n "$K8S_NAMESPACE" apply -f "$FILE"

          echo "Aguardando job db-migrate completar..."
          if ! kubectl -n "$K8S_NAMESPACE" wait --for=condition=complete --timeout=300s job/db-migrate; then
            echo "Job db-migrate falhou ou expirou. Exibindo logs e eventos:"
            kubectl -n "$K8S_NAMESPACE" describe job/db-migrate
            kubectl -n "$K8S_NAMESPACE" logs job/db-migrate --tail=200 || true
            kubectl -n "$K8S_NAMESPACE" get events --sort-by=.metadata.creationTimestamp -n "$K8S_NAMESPACE" | tail -n 20
            exit 1
          fi

          echo "Logs do job db-migrate:"
          kubectl -n "$K8S_NAMESPACE" logs job/db-migrate --tail=200