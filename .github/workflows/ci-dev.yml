name: CI Dev - Build, Push & Deploy

on:
  push:
    branches: [ "main" ]
  workflow_dispatch: {}

permissions:
  id-token: write    # OIDC
  contents: read

env:
  AWS_ACCOUNT_ID: ${{ vars.AWS_ACCOUNT_ID }}
  AWS_REGION: ${{ vars.AWS_REGION }}
  AWS_ROLE_TO_ASSUME: ${{ vars.AWS_ROLE_TO_ASSUME }}
  ECR_REPOSITORY: ${{ vars.ECR_REPOSITORY }}
  EKS_CLUSTER_NAME: ${{ vars.EKS_CLUSTER_NAME }}
  K8S_NAMESPACE: ${{ vars.K8S_NAMESPACE }}
  IMAGE_TAG: ${{ github.sha }}-amd64
  IMAGE_URI: ${{ vars.AWS_ACCOUNT_ID }}.dkr.ecr.${{ vars.AWS_REGION }}.amazonaws.com/${{ vars.ECR_REPOSITORY }}:${{ github.sha }}-amd64

jobs:
  build-push-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS creds (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Ensure ECR repository exists (idempotent)
        run: |
          aws ecr describe-repositories --repository-names "$ECR_REPOSITORY" >/dev/null 2>&1 || \
          aws ecr create-repository --repository-name "$ECR_REPOSITORY" >/dev/null

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build & Push (linux/amd64)
        run: |
          docker buildx build \
            --platform linux/amd64 \
            -t "$IMAGE_URI" \
            --push .

      - name: Install kubectl
        run: |
          curl -sSLo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Update kubeconfig (EKS)
        run: |
          aws eks update-kubeconfig --name "$EKS_CLUSTER_NAME" --region "$AWS_REGION"

      - name: Deploy - set image & wait rollout
        run: |
          # container name é "app" no Deployment
          kubectl -n "$K8S_NAMESPACE" set image deployment/lanchonete-app app="$IMAGE_URI"
          kubectl -n "$K8S_NAMESPACE" rollout status deployment/lanchonete-app --timeout=180s

      - name: Show deployed image
        run: |
          kubectl -n "$K8S_NAMESPACE" get deploy lanchonete-app -o jsonpath='{.spec.template.spec.containers[0].image}{"\n"}'

      - name: Install yq (YAML CLI)
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          yq --version

      - name: DB Migrate - fetch & render manifest (safe)
        run: |
          set -euo pipefail
          # baixa o YAML do repo privado via API com token do GitHub
          curl -fsSL \
            -H "Authorization: Bearer $GITHUB_TOKEN" \
            -H "Accept: application/vnd.github.raw" \
            "https://api.github.com/repos/good-burguer/lanchonete-infra/contents/k8s/jobs/db-migrate.yaml?ref=main" \
            -o db-migrate.yaml

          echo "---- HEAD db-migrate.yaml ----"
          sed -n '1,60p' db-migrate.yaml

          # valida se é mesmo um Job; evita aplicar HTML/erro
          if ! yq '.kind' db-migrate.yaml | grep -q '^Job$'; then
            echo "ERRO: db-migrate.yaml não parece um Job válido (kind != Job)"
            exit 1
          fi

          # altera a imagem apenas no documento do Job certo
          yq -i '
            select(.kind == "Job" and .metadata.name == "db-migrate")
            | .spec.template.spec.containers[0].image = strenv(IMAGE_URI)
          ' db-migrate.yaml

          echo "---- db-migrate.yaml (renderizado) ----"
          cat db-migrate.yaml

      - name: DB Migrate - apply and wait
        run: |
          set -euo pipefail
          kubectl -n "$K8S_NAMESPACE" delete job db-migrate --ignore-not-found=true
          kubectl -n "$K8S_NAMESPACE" apply -f db-migrate.yaml
          kubectl -n "$K8S_NAMESPACE" wait --for=condition=complete --timeout=300s job/db-migrate
          kubectl -n "$K8S_NAMESPACE" logs job/db-migrate --tail=200